{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true
      },
      "source": [
        "# 创建批处理推理服务\r\n",
        "\r\n",
        "假设一个健康诊所收集患者的全天测量值，将每位患者的详细信息保存在单独的文件中。然后在夜间，糖尿病预测模型可用于批量处理一整天的患者数据，生成第二天早上需要的预测，这样诊所就可以跟踪预测有患糖尿病风险的患者。借助 Azure 机器学习，可以通过创建批处理推理管道来做到这一点，你将在本练习中完成此操作。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 连接到工作区\r\n",
        "\r\n",
        "首先，请连接到你的工作区。\r\n",
        "\r\n",
        "> **备注**：如果尚未与 Azure 订阅建立经过身份验证的会话，则系统将提示你通过执行以下操作进行身份验证：单击链接，输入验证码，然后登录到 Azure。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import azureml.core\n",
        "from azureml.core import Workspace\n",
        "\n",
        "# 从保存的配置文件加载工作区\r\n",
        "ws = Workspace.from_config()\n",
        "print('Ready to use Azure ML {} to work with {}'.format(azureml.core.VERSION, ws.name))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 训练和注册模型\r\n",
        "\r\n",
        "现在我们来训练并注册模型以部署至分支推理管道中。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from azureml.core import Experiment\n",
        "from azureml.core import Model\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import joblib\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import roc_curve\n",
        "\n",
        "# 在工作区中创建 Azure ML 试验\r\n",
        "experiment = Experiment(workspace=ws, name='mslearn-train-diabetes')\n",
        "run = experiment.start_logging()\n",
        "print(\"Starting experiment:\", experiment.name)\n",
        "\n",
        "# 加载糖尿病数据集\r\n",
        "print(\"Loading Data...\")\n",
        "diabetes = pd.read_csv('data/diabetes.csv')\n",
        "\n",
        "# 分隔特征和标签\r\n",
        "X, y = diabetes[['Pregnancies','PlasmaGlucose','DiastolicBloodPressure','TricepsThickness','SerumInsulin','BMI','DiabetesPedigree','Age']].values, diabetes['Diabetic'].values\n",
        "\n",
        "# 将数据拆分为训练集和测试集\r\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=0)\n",
        "\n",
        "# 训练决策树模型\r\n",
        "print('Training a decision tree model')\n",
        "model = DecisionTreeClassifier().fit(X_train, y_train)\n",
        "\n",
        "# 计算精度\r\n",
        "y_hat = model.predict(X_test)\n",
        "acc = np.average(y_hat == y_test)\n",
        "print('Accuracy:', acc)\n",
        "run.log('Accuracy', np.float(acc))\n",
        "\n",
        "# 计算 AUC\r\n",
        "y_scores = model.predict_proba(X_test)\n",
        "auc = roc_auc_score(y_test,y_scores[:,1])\n",
        "print('AUC: ' + str(auc))\n",
        "run.log('AUC', np.float(auc))\n",
        "\n",
        "# 保存训练的模型\r\n",
        "model_file = 'diabetes_model.pkl'\n",
        "joblib.dump(value=model, filename=model_file)\n",
        "run.upload_file(name = 'outputs/' + model_file, path_or_stream = './' + model_file)\n",
        "\n",
        "# 完成运行\r\n",
        "run.complete()\n",
        "\n",
        "# 注册模型\r\n",
        "run.register_model(model_path='outputs/diabetes_model.pkl', model_name='diabetes_model',\n",
        "                   tags={'Training context':'Inline Training'},\n",
        "                   properties={'AUC': run.get_metrics()['AUC'], 'Accuracy': run.get_metrics()['Accuracy']})\n",
        "\n",
        "print('Model trained and registered.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 生成和上传批处理数据\r\n",
        "\r\n",
        "由于我们实际上没有人员齐备的诊所，无法获取用于此练习的患者新数据，因此你需要从糖尿病 CSV 文件生成随机样本，将这些数据上传至 Azure 机器学习工作区的数据存储中，并为其注册一个数据集。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from azureml.core import Datastore, Dataset\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# 设置默认数据存储\r\n",
        "ws.set_default_datastore('workspaceblobstore')\n",
        "default_ds = ws.get_default_datastore()\n",
        "\n",
        "# 枚举所有数据存储，指出默认项\r\n",
        "for ds_name in ws.datastores:\n",
        "    print(ds_name, \"- Default =\", ds_name == default_ds.name)\n",
        "\n",
        "# 加载糖尿病数据\r\n",
        "diabetes = pd.read_csv('data/diabetes2.csv')\n",
        "# 获取特征列（而非糖尿病标签）的 100 个项的样本\r\n",
        "sample = diabetes[['Pregnancies','PlasmaGlucose','DiastolicBloodPressure','TricepsThickness','SerumInsulin','BMI','DiabetesPedigree','Age']].sample(n=100).values\n",
        "\n",
        "# 创建文件夹\r\n",
        "batch_folder = './batch-data'\n",
        "os.makedirs(batch_folder, exist_ok=True)\n",
        "print(\"Folder created!\")\n",
        "\n",
        "# 将每个样本保存为单独的文件\r\n",
        "print(\"Saving files...\")\n",
        "for i in range(100):\n",
        "    fname = str(i+1) + '.csv'\n",
        "    sample[i].tofile(os.path.join(batch_folder, fname), sep=\",\")\n",
        "print(\"files saved!\")\n",
        "\n",
        "# 将文件上传到默认数据存储\r\n",
        "print(\"Uploading files to datastore...\")\n",
        "default_ds = ws.get_default_datastore()\n",
        "default_ds.upload(src_dir=\"batch-data\", target_path=\"batch-data\", overwrite=True, show_progress=True)\n",
        "\n",
        "# 为输入数据注册数据集\r\n",
        "batch_data_set = Dataset.File.from_files(path=(default_ds, 'batch-data/'), validate=False)\n",
        "try:\n",
        "    batch_data_set = batch_data_set.register(workspace=ws, \n",
        "                                             name='batch-data',\n",
        "                                             description='batch data',\n",
        "                                             create_new_version=True)\n",
        "except Exception as ex:\n",
        "    print(ex)\n",
        "\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 创建计算\r\n",
        "\r\n",
        "我们需要管道的计算上下文，因此我们将使用以下代码指定一个 Azure 机器学习计算群集（如果不存在，则会创建该群集）。\r\n",
        "\r\n",
        "> **重要事项**：在运行以下代码之前，请先将代码中的 your-compute-cluster **更改为你的计算群集的名称！群集名称必须是长度在 2 到 16 个字符之间的全局唯一名称。有效字符是字母、数字和 - 字符。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from azureml.core.compute import ComputeTarget, AmlCompute\n",
        "from azureml.core.compute_target import ComputeTargetException\n",
        "\n",
        "cluster_name = \"your-compute-cluster\"\n",
        "\n",
        "try:\n",
        "    # Check for existing compute target\n",
        "    inference_cluster = ComputeTarget(workspace=ws, name=cluster_name)\n",
        "    print('Found existing cluster, use it.')\n",
        "except ComputeTargetException:\n",
        "    # If it doesn't already exist, create it\n",
        "    try:\n",
        "        compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_DS11_V2', max_nodes=2)\n",
        "        inference_cluster = ComputeTarget.create(ws, cluster_name, compute_config)\n",
        "        inference_cluster.wait_for_completion(show_output=True)\n",
        "    except Exception as ex:\n",
        "        print(ex)\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 创建用于批处理推理的管道\r\n",
        "\r\n",
        "现在，我们可以开始定义将用于批量推理的管道。我们的管道将需要 Python 代码来执行批量推理，因此我们来创建一个可存储该管道使用的所有文件的文件夹："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "# 为试验文件创建文件夹\r\n",
        "experiment_folder = 'batch_pipeline'\n",
        "os.makedirs(experiment_folder, exist_ok=True)\n",
        "\n",
        "print(experiment_folder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "现在，我们将创建一个 Python 脚本来完成实际工作，并将其保存在管道文件夹中："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile $experiment_folder/batch_diabetes.py\n",
        "import os\n",
        "import numpy as np\n",
        "from azureml.core import Model\n",
        "import joblib\n",
        "\n",
        "\n",
        "def init():\n",
        "    # Runs when the pipeline step is initialized\n",
        "    global model\n",
        "\n",
        "    # load the model\n",
        "    model_path = Model.get_model_path('diabetes_model')\n",
        "    model = joblib.load(model_path)\n",
        "\n",
        "\n",
        "def run(mini_batch):\n",
        "    # This runs for each batch\n",
        "    resultList = []\n",
        "\n",
        "    # process each file in the batch\n",
        "    for f in mini_batch:\n",
        "        # Read the comma-delimited data into an array\n",
        "        data = np.genfromtxt(f, delimiter=',')\n",
        "        # Reshape into a 2-dimensional array for prediction (model expects multiple items)\n",
        "        prediction = model.predict(data.reshape(1, -1))\n",
        "        # Append prediction to results\n",
        "        resultList.append(\"{}: {}\".format(os.path.basename(f), prediction[0]))\n",
        "    return resultList"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "接下来，我们将定义一个包括脚本所需的依赖项的运行上下文"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from azureml.core import Environment\n",
        "from azureml.core.runconfig import DEFAULT_CPU_IMAGE\n",
        "from azureml.core.runconfig import CondaDependencies\n",
        "\n",
        "# 添加模型所需的依赖项\r\n",
        "# 对于 scikit-learn 模型，你需要 scikit-learn\r\n",
        "# 对于并行管道步骤，你需要 azureml-core 和 azureml-dataprep[fuse]\r\n",
        "cd = CondaDependencies.create(conda_packages=['scikit-learn','pip'],\n",
        "                              pip_packages=['azureml-defaults','azureml-core','azureml-dataprep[fuse]'])\n",
        "\n",
        "batch_env = Environment(name='batch_environment')\n",
        "batch_env.python.conda_dependencies = cd\n",
        "batch_env.docker.base_image = DEFAULT_CPU_IMAGE\n",
        "print('Configuration ready.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "你将使用管道来运行批量预测脚本、从输入数据生成预测以及将结果作为文本文件保存在 outputs 文件夹中。为此，可以使用 **ParallelRunStep**，通过它可以并行处理批量数据，并将结果整理到名为 *parallel_run_step.txt* 的单个输出文件中。\r\n",
        "\r\n",
        "> **备注**：可能会显示 *“*不建议使用“已启用””* 警告，你可以忽略此警告。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from azureml.pipeline.steps import ParallelRunConfig, ParallelRunStep\n",
        "from azureml.pipeline.core import PipelineData\n",
        "from azureml.core.runconfig import DockerConfiguration\n",
        "\n",
        "default_ds = ws.get_default_datastore()\n",
        "\n",
        "output_dir = PipelineData(name='inferences', \n",
        "                          datastore=default_ds, \n",
        "                          output_path_on_compute='diabetes/results')\n",
        "\n",
        "parallel_run_config = ParallelRunConfig(\n",
        "    source_directory=experiment_folder,\n",
        "    entry_script=\"batch_diabetes.py\",\n",
        "    mini_batch_size=\"5\",\n",
        "    error_threshold=10,\n",
        "    output_action=\"append_row\",\n",
        "    environment=batch_env,\n",
        "    compute_target=inference_cluster,\n",
        "    node_count=2)\n",
        "\n",
        "parallelrun_step = ParallelRunStep(\n",
        "    name='batch-score-diabetes',\n",
        "    parallel_run_config=parallel_run_config,\n",
        "    inputs=[batch_data_set.as_named_input('diabetes_batch')],\n",
        "    output=output_dir,\n",
        "    arguments=[],\n",
        "    allow_reuse=True\n",
        ")\n",
        "\n",
        "print('Steps defined')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "现在应将此步骤放入管道中并运行它。\r\n",
        "\r\n",
        "> **备注**：这可能需要一些时间！"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "from azureml.core import Experiment\n",
        "from azureml.pipeline.core import Pipeline\n",
        "\n",
        "pipeline = Pipeline(workspace=ws, steps=[parallelrun_step])\n",
        "pipeline_run = Experiment(ws, 'mslearn-diabetes-batch').submit(pipeline)\n",
        "pipeline_run.wait_for_completion(show_output=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "管道运行完成后，生成的预测将保存在与管道中第一个（也是唯一一个）步骤相关联的试验输出中。你可按照以下步骤对其进行检索："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import shutil\n",
        "\n",
        "# 如果之前的运行遗留了本地结果文件夹，请删除该文件夹\r\n",
        "shutil.rmtree('diabetes-results', ignore_errors=True)\n",
        "\n",
        "# 获取第一步骤的运行并下载其输出\r\n",
        "prediction_run = next(pipeline_run.get_children())\n",
        "prediction_output = prediction_run.get_output_data('inferences')\n",
        "prediction_output.download(local_path='diabetes-results')\n",
        "\n",
        "# 遍历文件夹层次结构并找到结果文件\r\n",
        "for root, dirs, files in os.walk('diabetes-results'):\n",
        "    for file in files:\n",
        "        if file.endswith('parallel_run_step.txt'):\n",
        "            result_file = os.path.join(root,file)\n",
        "\n",
        "# 清理输出格式\r\n",
        "df = pd.read_csv(result_file, delimiter=\":\", header=None)\n",
        "df.columns = [\"File\", \"Prediction\"]\n",
        "\n",
        "# 显示前 20 个结果\r\n",
        "df.head(20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 发布管道并使用其 REST 接口\r\n",
        "\r\n",
        "拥有用于批量推理的工作管道后，你可以发布它并使用 REST 终结点从应用程序中运行它。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "published_pipeline = pipeline_run.publish_pipeline(\n",
        "    name='diabetes-batch-pipeline', description='Batch scoring of diabetes data', version='1.0')\n",
        "\n",
        "published_pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "请注意，已发布的管道具有一个终结点，可在 Azure 门户中看到此终结点。还可以将其视为已发布管道对象的属性："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rest_endpoint = published_pipeline.endpoint\n",
        "print(rest_endpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "要使用该终结点，客户端应用程序需要通过 HTTP 进行 REST 调用。此请求必须经过身份验证，因此需要授权标头。为了测试这一点，我们将使用当前连接到 Azure 工作区的授权标头（可使用以下代码获取该标头）：\r\n",
        "\r\n",
        "> **备注**：实际的应用程序需要使用服务主体进行身份验证。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from azureml.core.authentication import InteractiveLoginAuthentication\n",
        "\n",
        "interactive_auth = InteractiveLoginAuthentication()\n",
        "auth_header = interactive_auth.get_authentication_header()\n",
        "print('Authentication header ready.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "现在可以调用 REST 接口。管道异步运行，因此我们将获得一个标识符，可用于跟踪运行中的管道试验："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "rest_endpoint = published_pipeline.endpoint\n",
        "response = requests.post(rest_endpoint, \n",
        "                         headers=auth_header, \n",
        "                         json={\"ExperimentName\": \"mslearn-diabetes-batch\"})\n",
        "run_id = response.json()[\"Id\"]\n",
        "run_id"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "由于具有运行 ID，因此可以使用“**RunDetails**”小组件查看运行中的试验："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from azureml.pipeline.core.run import PipelineRun\n",
        "from azureml.widgets import RunDetails\n",
        "\n",
        "published_pipeline_run = PipelineRun(ws.experiments['mslearn-diabetes-batch'], run_id)\n",
        "\n",
        "# 阻止至运行完成\r\n",
        "published_pipeline_run.wait_for_completion(show_output=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "等待管道完成运行，然后运行以下单元格以查看结果。\r\n",
        "\r\n",
        "如前所述，结果位于第一个管道步骤的输出中："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import shutil\n",
        "\n",
        "# 如果之前的运行遗留了本地结果文件夹，请删除该文件夹\r\n",
        "shutil.rmtree('diabetes-results', ignore_errors=True)\n",
        "\n",
        "# 获取第一步骤的运行并下载其输出\r\n",
        "prediction_run = next(pipeline_run.get_children())\n",
        "prediction_output = prediction_run.get_output_data('inferences')\n",
        "prediction_output.download(local_path='diabetes-results')\n",
        "\n",
        "# 遍历文件夹层次结构并找到结果文件\r\n",
        "for root, dirs, files in os.walk('diabetes-results'):\n",
        "    for file in files:\n",
        "        if file.endswith('parallel_run_step.txt'):\n",
        "            result_file = os.path.join(root,file)\n",
        "\n",
        "# 清理输出格式\r\n",
        "df = pd.read_csv(result_file, delimiter=\":\", header=None)\n",
        "df.columns = [\"File\", \"Prediction\"]\n",
        "\n",
        "# 显示前 20 个结果\r\n",
        "df.head(20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "现在你具有一个可用于批量处理每日患者数据的管道。\r\n",
        "\r\n",
        "**详细信息**：若要详细了解如何使用管道进行批量推理，请参阅 Azure 机器学习文档中的[如何运行批量预测](https://docs.microsoft.com/azure/machine-learning/how-to-run-batch-predictions)。"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.6 - AzureML",
      "language": "python",
      "name": "python3-azureml"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}